---
title: "IE 582 HW2"
author: "Dürdane Karabacak"
date: "01 11 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###TASK 1-Multiple Instance Learning

##QUESTION 1 
#MDS
```{r TASK1-Q1-MDS}
require(MASS)
require(data.table)
library(factoextra)

#Reading musk data

musk<-read.csv("C:/Users/DÜRDANE KARABACAK/Desktop/IE 582 HW/HW 2/Musk1.csv")
baginfo<-musk[,c(1,2)]
musk1<-musk[,-c(1,2)]

#Calculation of distance matrix
dist<-dist(musk1,method = "euclidean", diag = FALSE, upper = FALSE,p=2)
distancematrix<-as.matrix(dist)
distancematrix[is.na(distancematrix)]=0
distancematrix[1:9,1:9]

#Apply MDS on distance matrix
fit<-cmdscale(distancematrix)
fit<-data.table(fit)
fit[,c("BagClass", "BagID")]<-baginfo[,1:2]
fit

#Classify musk data which is in class 1 and class 0 
MuskData<-fit[BagClass==1]
NonmuskData<-fit[BagClass==0]
MuskData
NonmuskData

#Plots 

plot(MuskData$V1,MuskData$V2,main='MDS Method',col="red",type = "p",xlab='', ylab='')
points(NonmuskData$V1,NonmuskData$V2,col="blue")




```
##COMMENTS

```{r TASK1-Q1-PCA}

musk1<-musk[,-c(1,2)]
#To show musk data for initial 5 columns
plot(musk1[,1:5],col=2,pch=".",cex=7)
summary(musk1)

#Calculation of correlation matrix

cormatrix<-cor(musk1)
View(cormatrix)

#Apply PCA 

pca <- princomp(musk1, cor=T)

plot(pca,main = "Variance explained by components",col="darkblue")
plot(pca$scores[,1],pca$scores[,2])

loadings<- unclass(pca$loadings[,1:3])



```


##COMMENTS
Here after applying PCA to musk data mean that:
1- Applying PCA with 2 Dimensions protects approximately %74 percent of the variance but this is not so enough to cover all variance. Therefore, we need more dimensions. With first four principal components 94 % of the variance data can be saved.
```{r TASK 1-Q2}
#read musk data

musk<-read.csv("C:/Users/DÜRDANE KARABACAK/Desktop/IE 582 HW/HW 2/Musk1.csv")
names(musk)[1] <- "BagClass"
names(musk)[2] <- "BagID"

#taking means of rows with the same BagID
mean_table<-aggregate(.~BagID, data=musk, mean)

##MDS
baginfo<-mean_table[,(1:2)]
mean_table<-mean_table[,-(1:2)]

#Distance matrix

dist<-dist(mean_table,method = "euclidean", diag = FALSE, upper = FALSE,p=2)
distancematrix<-as.matrix(dist)
distancematrix[is.na(distancematrix)]=0
View(distancematrix)

#Apply mds to the agreegated data
Mds_mean<-cmdscale(distancematrix)
View(Mds_mean)
Mds_mean<-data.table(Mds_mean)
Mds_mean<-cbind(Mds_mean,baginfo)

#Clarify the musk's classes

MuskData<-Mds_mean[BagClass==1]
NonmuskData<-Mds_mean[BagClass==0]

#Plots

plot(MuskData$V1,MuskData$V2,main='MDS Method-Data dispersion and classes',col="red",type = "p",xlab='', ylab='')
points(NonmuskData$V1,NonmuskData$V2,col="blue")



##PCA 

plot(mean_table[,1:5],col=2,pch=".",cex=7)
summary(mean_table)

#correlation matrix

cormatrix<-cor(mean_table)
View(cormatrix)

#Apply PCA 
#Since mean table has 92 rows which is less than columns, this caused an compilation error so that i made a sampling with size 92 on the mean table data below

set.seed(1)
x<-sample(1:166,92,replace=T)
pca_data<-mean_table[,x]
pca_mean <- princomp(pca_data, cor=T)

loadings<- unclass(pca_mean$loadings[,1:3])

#Plots 

plot(pca_mean,main = "Variance explained by components on mean table",col="pink")

plot(pca_mean$scores[,1],pca_mean$scores[,2])

barplot(pca_mean$scores[,1])
barplot(pca_mean$scores[,2])

```


###TASK 2 

##QUESTION 1

```{r TASK 2-Q1}
library(jpeg)
library(fitdistrplus)
library(graphics)
library(grDevices)

#Question 1 

img<-readJPEG("D:/ROMA.jpg")
grayimg<-readJPEG("D:/ROMA-siyah beyaz.jpg")
str(img)
dim(img)
class(img)
View(img)
plot(img)
if(exists("rasterImage")){
  plot(1:256, type='n', xlab = "", ylab = "")
  rasterImage(img,1,1,256,256,interpolate = FALSE)
}

#Plot of channels side by side without noise 

# make labels and margins smaller
par(cex=0.5, mai=c(0.2,0.2,0.2,0.2))
# define area for the histogram
par(fig=c(0,0.33,0.33,0.66))
image(img[,,1],col=hcl.colors(12, "Reds"),main="R Channel")
# define area for the boxplot
par(fig=c(0.33,0.66,0.33,0.66), new=TRUE)
image(img[,,2],col=hcl.colors(12, "Greens"),main="G Channel")
# define area for the stripchart
par(fig=c(0.66,0.99,0.33,0.66), new=TRUE)
image(img[,,3],col=hcl.colors(12, "Blues"),main="B Channel")




```



##QUESTION 2


```{r TASK 2-Q2}
noiseimg<-array(dim = c(256,256,3))
for (j in 1:3){
  min<-min(img[,,j])
  max<-max(img[,,j])
  for (i in 1:256) {
    for (k in 1:256) {
      noiseimg[i,k,j]<- img[i,k,j]* runif(1,max = max,min = min)
    }
    
  }
}

if(exists("rasterImage")){
  plot(1:256, type='n', xlab = "", ylab = "",main = "Image with noise")
  rasterImage(noiseimg,1,1,256,256,interpolate = FALSE)
}


'Since uniform random variables are generally between (0,1), pixel values were multiplied by 
a number which is less than one. This results for each pixel to take lower values 
which make colors of images more darkened because 0 value signifies black color.
'


#Plot of channels side by side 

# make labels and margins smaller
par(cex=0.5, mai=c(0.2,0.2,0.2,0.2))
# define area for the histogram
par(fig=c(0,0.33,0.33,0.66))
image(noiseimg[,,1],col=hcl.colors(12, "Reds"),main="R Channel")
# define area for the boxplot
par(fig=c(0.33,0.66,0.33,0.66), new=TRUE)
image(noiseimg[,,2],col=hcl.colors(12, "Greens"),main="G Channel")
# define area for the stripchart
par(fig=c(0.66,0.99,0.33,0.66), new=TRUE)
image(noiseimg[,,3],col=hcl.colors(12, "Blues"),main="B Channel")




```
##Comments
Since uniform random variables are generally between (0,1), pixel values were multiplied by 
a number which is less than one. This results for each pixel to take lower values 
which make colors of images more darkened because 0 value signifies black color.


```{r TASK2-Q3-}
#Contunuing in other R file

```

